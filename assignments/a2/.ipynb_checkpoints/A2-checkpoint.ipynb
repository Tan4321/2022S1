{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9pWsPAt0dCh"
   },
   "source": [
    "# Assignment 2 - Find complex answers to medical questions\n",
    "\n",
    "**Submission deadline: Friday 22 April, 5pm.** \n",
    "\n",
    "Late submissions **will not be accepted** without an approved [Special Consideration](http://from.mq.edu.au/MT0X0E0FUrrU200rm0JB0U0) request.  Assessments submitted after the due date will receive a mark of **zero**.\n",
    "\n",
    "**Assessment marks: 20 marks (20% of the total unit assessment)**\n",
    "\n",
    "In this assignment we will work on a task of \"query-focused summarisation\" on medical questions where the goal is, given a medical question and a list of sentences extracted from relevant medical publications, to determine which of these sentences from the list can be used as part of the answer to the question.\n",
    "\n",
    "We will use data that has been derived from the **BioASQ challenge** (http://www.bioasq.org/), after some data manipulation to make it easier to process for this assignment. The BioASQ challenge organises several \"shared tasks\", including a task on biomedical semantic question answering which we are using here. The data are in the file `bioasq10_labelled.csv`, which is part of the zip file provided. Each row of the file has a question, a sentence text, and a label that indicates whether the sentence text is part of the answer to the question (1) or not (0).\n",
    "\n",
    "The following code uses pandas to store the file `bioasq10_labelled.csv` in a data frame and show the first rows of data. For this code to run, first you need to unzip the file `data.zip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>sentid</th>\n",
       "      <th>question</th>\n",
       "      <th>sentence text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>Hirschsprung disease (HSCR) is a multifactoria...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>In this study, we review the identification of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>The majority of the identified genes are relat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>The non-Mendelian inheritance of sporadic non-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>Coding sequence mutations in e.g.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>RET, GDNF, EDNRB, EDN3, and SOX10 lead to long...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>Furthermore, mutations in the RET gene are res...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>For almost all of the identified HSCR genes in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>Therefore, HSCR has become a model for a compl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>Hirschsprung disease (HSCR) is a multifactori...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>The inheritance of Hirschsprung disease is ge...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    qid  sentid                                           question  \\\n",
       "0     0       0  Is Hirschsprung disease a mendelian or a multi...   \n",
       "1     0       1  Is Hirschsprung disease a mendelian or a multi...   \n",
       "2     0       2  Is Hirschsprung disease a mendelian or a multi...   \n",
       "3     0       3  Is Hirschsprung disease a mendelian or a multi...   \n",
       "4     0       4  Is Hirschsprung disease a mendelian or a multi...   \n",
       "5     0       5  Is Hirschsprung disease a mendelian or a multi...   \n",
       "6     0       6  Is Hirschsprung disease a mendelian or a multi...   \n",
       "7     0       7  Is Hirschsprung disease a mendelian or a multi...   \n",
       "8     0       8  Is Hirschsprung disease a mendelian or a multi...   \n",
       "9     0       9  Is Hirschsprung disease a mendelian or a multi...   \n",
       "10    0      10  Is Hirschsprung disease a mendelian or a multi...   \n",
       "\n",
       "                                        sentence text  label  \n",
       "0   Hirschsprung disease (HSCR) is a multifactoria...      0  \n",
       "1   In this study, we review the identification of...      1  \n",
       "2   The majority of the identified genes are relat...      1  \n",
       "3   The non-Mendelian inheritance of sporadic non-...      1  \n",
       "4                   Coding sequence mutations in e.g.      0  \n",
       "5   RET, GDNF, EDNRB, EDN3, and SOX10 lead to long...      0  \n",
       "6   Furthermore, mutations in the RET gene are res...      0  \n",
       "7   For almost all of the identified HSCR genes in...      0  \n",
       "8   Therefore, HSCR has become a model for a compl...      0  \n",
       "9    Hirschsprung disease (HSCR) is a multifactori...      0  \n",
       "10   The inheritance of Hirschsprung disease is ge...      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = pd.read_csv(\"bioasq10b_labelled.csv\")\n",
    "dataset.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of the CSV file are:\n",
    "\n",
    "* `qid`: an ID for a question. Several rows may have the same question ID, as we can see above.\n",
    "* `sentid`: an ID for a sentence.\n",
    "* `question`: The text of the question. In the above example, the first rows all have the same question: \"Is Hirschsprung disease a mendelian or a multifactorial disorder?\"\n",
    "* `sentence text`: The text of the sentence.\n",
    "* `label`: 1 if the sentence is a part of the answer, 0 if the sentence is not part of the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6xqxCmR0dCk"
   },
   "source": [
    "# Task 1 (5 marks): Data preparation\n",
    "\n",
    "Partition the data into the training, dev_test, and test sets using the proportions 6:2:2. That is, 60% of the questions must be in the training set, 20% must be in the dev_test set, and the remaining 20% in the test set. Make sure that you partition based on the questions, not on the rows. With this we mean that all the sentences related to a question must be in one file only. In other words, there must not be sentences from the same question in, say, the training and the test data.\n",
    "\n",
    "Also, make sure that you implement a random partition.\n",
    "\n",
    "Save the partitions as the files `training.csv`, `dev_test.csv`, and `test.csv`, so that they can be used by other people.\n",
    "\n",
    "The breakdown of marks is as follows:\n",
    "\n",
    "* **1 mark** if your explanation answers the following question correctly: Why do we want to split the partition on the questions, and not on the rows?\n",
    "* **1 mark** if the code partitions the data on the questions randomly and according to the split 6:2:2.\n",
    "* **1 mark** if your code generates partitions that have similar balance of labels and you demonstrate that they are similar.\n",
    "* **1 mark** if the partitions are saved as the CSV files `training.csv`, `dev_test.csv`, and `test.csv`.\n",
    "* **1 mark** for good coding and documentation in this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fhqUtqJL0dCm"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Su2UC0Ti0dCo",
    "outputId": "4a705b65-e2e5-4415-a53b-a97b79bc2eaf"
   },
   "outputs": [],
   "source": [
    "dataset= dataset.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split question to 6:2:2 in respect to the question \n",
    "\n",
    "train_set = dataset[:600]\n",
    "devtest_set =dataset[600:800]\n",
    "test_set=dataset[800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.298118916895433"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean(dataset[\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28833333333333333"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean(train_set[\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean(devtest_set[\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2982688905736669"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean(test_set[\"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_csv('train_set.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "devtest_set.to_csv('devtest_set.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.to_csv('test_set.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbUJWlD_0dCv"
   },
   "source": [
    "# Task 2 (5 marks): Cosine similarity\n",
    "\n",
    "Use the files `training.csv`, `dev_test.csv`, and `test.csv` provided by us in the file `data.zip` (so that any possible errors that you may have introduced in task 1 do not propagate to this task and following tasks).\n",
    "\n",
    "Implement a simple text summariser that is based on the cosine similarity between the question and the text. Use the following function signature.\n",
    "\n",
    "```{python}\n",
    "def cosine_summariser(csvfile, questionids, n=5):\n",
    "   \"\"\"Return the IDs of the n sentences that have the highest cosine similarity\n",
    "    with the question. The input questionids is a list of question ids. The \n",
    "    output is a list of lists of sentence ids\n",
    "    >>> cosine_summariser('test.csv', [3, 11], 3)\n",
    "    [[3, 1, 4], [12, 4, 13]]\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "To obtain the text vectors, use sklearn's tf.idf libraries this way:\n",
    "\n",
    "* Use all the defaults from the TfidfVectorizer instance, except for `stop_words=\"english\"` and `max_features=10000`. The latter option will restrict the vocabulary size to 10,000. This will speed up the computations and reduce the memory footprint in the subsequent tasks.\n",
    "* Use the `fit` method on the text of `training.csv`. In your documentation, please explain and justify what decision choices you made to select the correct text: would you use the question text only, the sentence text, or both?\n",
    "\n",
    "Evaluate the summariser by reporting the mean F1 score on each of the three CSV files `training.csv`, `devtest.csv`, and `test.csv`, for $n=5$. To calculate the mean F1 score, do this:\n",
    "\n",
    "1. For each question ID in the file, calculate the F1 score by comparing the result of your cosine summariser and the given labels. Feel free to use sklearn's functions to compute the F1 score, or implement your own version of the F1 scoring function if you prefer.\n",
    "2. Calculate the mean of the F1 scores calculated in step 1.\n",
    "\n",
    "Find the value of $n$ that returns the highest mean F1 score on the dev_test data.\n",
    "\n",
    "The breakdown of marks is as follows:\n",
    "\n",
    "* **1 mark** if the code generates the tf.idf vectors correctly. The explanations that justify the decisions made are reasonable. In particular, explain and justify what information you used to fit tf.idf.\n",
    "* **1 mark** if the code calculates cosine similarity correctly.\n",
    "* **1 mark** if the code returns the IDs of the $n$ sentences that have the highest cosine similarity with the question.\n",
    "* **1 mark** if the notebook reports the F1 scores of the dev_test file and identifies the value of $n$ that gives the highest score on the dev_test file.\n",
    "* **1 mark** for good coding and documentation in this task. In particular, comment on the reason why you think the value of $n$ that gives highest F1 has that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "UAr6x5Zc0dCw",
    "outputId": "7fa58ee8-d420-47ce-e925-c37950857924"
   },
   "outputs": [],
   "source": [
    "# Write your code and answers here. Feel free to add more code and markdown cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VTTgRnN0dC4"
   },
   "source": [
    "# Task 3 (5 marks): Simple NN\n",
    "\n",
    "Use the files `training.csv`, `dev_test.csv`, and `test.csv` provided by us.\n",
    "\n",
    "Implement a simple TensorFlow-Keras neural model that has the following sequence of layers:\n",
    "\n",
    "1. An input layer that will accept the tf.idf of the sentence text (we will ignore the question text in this task). Use the TfidfVectorizer instance that you have fitted in task 2.\n",
    "2. A hidden layer and a relu activation function. You need to determine the size of the hidden layer.\n",
    "3. An output layer with one cell. The output layer will classify the input text (binary classification).\n",
    "\n",
    "Train the model with the training data and use the dev_test set to determine a good size of the hidden layer. \n",
    "\n",
    "With the model that you have trained, and implement a summariser that returns the $n$ sentences with highest predicted score. Use the following function signature:\n",
    "\n",
    "```{python}\n",
    "def nn_summariser(csvfile, questionids, n=5):\n",
    "   \"\"\"Return the IDs of the n sentences that have the highest predicted score. The input questionids is a list of question ids. The \n",
    "    output is a list of lists of sentence ids\n",
    "    >>> cosine_summariser('test.csv', [3, 11], 3)\n",
    "    [[2, 1, 3], [7, 14, 10]]\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "Report the final results using the test set. Remember: use the test set to report the final results of the best system only.\n",
    "\n",
    "Based on your experiments, comment on whether this system is better than the system developed in task 2. To make this task less time-consuming, focus only on $n=5$.\n",
    "\n",
    "The breakdown of marks is as follows:\n",
    "\n",
    "* **1 mark** if the NN model has the correct layers, the correct activation functions, and the correct loss function.\n",
    "* **1 mark** if the code passes the tf.idf information of the text to the model correctly.\n",
    "* **1 mark** if the code returns the IDs of the $n$ sentences that have the highest prediction score in the given question.\n",
    "* **1 mark** if the notebook reports the F1 scores of the test sets and comments on the results.\n",
    "* **1 mark** for good coding and documentation in this task. In particular, the code and results must include evidence that shows your choice of best size of the hidden layer. The explanations must be clear and concise. To make this task less time-consuming, use $n=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "uY6sDbUn0dC6"
   },
   "outputs": [],
   "source": [
    "devtest_test=pd.read_csv(\"devtest_set.csv\")\n",
    "train_test=pd.read_csv(\"train_set.csv\")\n",
    "test_test=pd.read_csv(\"test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "devtest_set=list(devtest_test.itertuples(index=False,name=None))\n",
    "train_set=list(train_test.itertuples(index=False,name=None))\n",
    "test_set=list(test_test.itertuples(index=False,name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer(input='contents', max_features=10000,stop_words=\"english\")\n",
    "train_tfidf2=tfidf.fit_transform([t[3]for t in train_set]).toarray()\n",
    "train_qid=[t[4]for t in train_set]\n",
    "devtest_tfidf2=tfidf.transform([u[3]for u in devtest_set]).toarray()\n",
    "devtest_qid=[u[4]for u in devtest_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.8.0-cp38-cp38-macosx_10_14_x86_64.whl (217.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 217.4 MB 10.0 kB/s eta 0:00:01    |█████████▊                      | 65.9 MB 34.0 MB/s eta 0:00:05     |█████████████▊                  | 92.9 MB 75.7 MB/s eta 0:00:02     |███████████████████▊            | 133.8 MB 346 kB/s eta 0:04:02     |████████████████████▋           | 140.1 MB 29.2 MB/s eta 0:00:03     |███████████████████████         | 156.2 MB 29.2 MB/s eta 0:00:03     |██████████████████████████▎     | 178.6 MB 12.5 MB/s eta 0:00:04\n",
      "\u001b[?25hCollecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 12.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting libclang>=9.0.1\n",
      "  Downloading libclang-13.0.0-py2.py3-none-macosx_10_9_x86_64.whl (13.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.0 MB 15.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /Users/tanzimul/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.20.1)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting keras<2.9,>=2.8.0rc0\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 26.5 MB/s eta 0:00:01     |█████████████████████▍          | 921 kB 26.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /Users/tanzimul/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.25.0-cp38-cp38-macosx_10_14_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 21.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast>=0.2.1\n",
      "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/tanzimul/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 6.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 22.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /Users/tanzimul/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 33.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.20.1-cp38-cp38-macosx_10_9_x86_64.whl (962 kB)\n",
      "\u001b[K     |████████████████████████████████| 962 kB 31.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/tanzimul/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (52.0.0.post20210125)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 4.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.44.0-cp38-cp38-macosx_10_10_x86_64.whl (4.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3 MB 35.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /Users/tanzimul/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/tanzimul/opt/anaconda3/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.36.2)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 15.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /Users/tanzimul/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/tanzimul/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5 MB 34.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 45.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\n",
      "\u001b[K     |████████████████████████████████| 156 kB 25.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 26.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/tanzimul/opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.4.1)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 5.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /Users/tanzimul/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/tanzimul/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/tanzimul/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tanzimul/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2020.12.5)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 19.0 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=2be2dd1a1ebaf9d5abaf9495875fcb259fa42ce59ad78e0dac1bd4ef35037646\n",
      "  Stored in directory: /Users/tanzimul/Library/Caches/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, tf-estimator-nightly, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 3.10.0\n",
      "    Uninstalling importlib-metadata-3.10.0:\n",
      "      Successfully uninstalled importlib-metadata-3.10.0\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.0.0 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.6 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.44.0 importlib-metadata-4.11.3 keras-2.8.0 keras-preprocessing-1.1.2 libclang-13.0.0 markdown-3.3.6 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.20.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.25.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 3)                 13041     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,045\n",
      "Trainable params: 13,045\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as Tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers \n",
    "\n",
    "simple_nn= models.Sequential()\n",
    "simple_nn.add(layers.Dense(3, activation='relu',input_shape=(len(tfidf.get_feature_names()),)))\n",
    "simple_nn.add(layers.Dense(1,activation='sigmoid'))\n",
    "simple_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "19/19 [==============================] - 1s 22ms/step - loss: 0.6888 - accuracy: 0.6917 - val_loss: 0.6847 - val_accuracy: 0.7200\n",
      "Epoch 2/70\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.6804 - accuracy: 0.7117 - val_loss: 0.6781 - val_accuracy: 0.7200\n",
      "Epoch 3/70\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.6727 - accuracy: 0.7117 - val_loss: 0.6722 - val_accuracy: 0.7200\n",
      "Epoch 4/70\n",
      "19/19 [==============================] - 0s 11ms/step - loss: 0.6648 - accuracy: 0.7117 - val_loss: 0.6660 - val_accuracy: 0.7200\n",
      "Epoch 5/70\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.6571 - accuracy: 0.7117 - val_loss: 0.6604 - val_accuracy: 0.7200\n",
      "Epoch 6/70\n",
      "19/19 [==============================] - 0s 12ms/step - loss: 0.6489 - accuracy: 0.7117 - val_loss: 0.6545 - val_accuracy: 0.7200\n",
      "Epoch 7/70\n",
      "19/19 [==============================] - 0s 11ms/step - loss: 0.6409 - accuracy: 0.7117 - val_loss: 0.6491 - val_accuracy: 0.7200\n",
      "Epoch 8/70\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.6328 - accuracy: 0.7117 - val_loss: 0.6437 - val_accuracy: 0.7200\n",
      "Epoch 9/70\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.6247 - accuracy: 0.7117 - val_loss: 0.6393 - val_accuracy: 0.7200\n",
      "Epoch 10/70\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.6173 - accuracy: 0.7117 - val_loss: 0.6348 - val_accuracy: 0.7200\n",
      "Epoch 11/70\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.6095 - accuracy: 0.7117 - val_loss: 0.6305 - val_accuracy: 0.7200\n",
      "Epoch 12/70\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.6015 - accuracy: 0.7117 - val_loss: 0.6263 - val_accuracy: 0.7200\n",
      "Epoch 13/70\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.5934 - accuracy: 0.7117 - val_loss: 0.6224 - val_accuracy: 0.7200\n",
      "Epoch 14/70\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.5851 - accuracy: 0.7117 - val_loss: 0.6187 - val_accuracy: 0.7200\n",
      "Epoch 15/70\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.5772 - accuracy: 0.7117 - val_loss: 0.6154 - val_accuracy: 0.7200\n",
      "Epoch 16/70\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.5693 - accuracy: 0.7117 - val_loss: 0.6123 - val_accuracy: 0.7200\n",
      "Epoch 17/70\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.5614 - accuracy: 0.7117 - val_loss: 0.6101 - val_accuracy: 0.7200\n",
      "Epoch 18/70\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.5535 - accuracy: 0.7117 - val_loss: 0.6079 - val_accuracy: 0.7200\n",
      "Epoch 19/70\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.5457 - accuracy: 0.7117 - val_loss: 0.6058 - val_accuracy: 0.7200\n",
      "Epoch 20/70\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.5378 - accuracy: 0.7117 - val_loss: 0.6042 - val_accuracy: 0.7200\n",
      "Epoch 21/70\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.5300 - accuracy: 0.7117 - val_loss: 0.6028 - val_accuracy: 0.7200\n",
      "Epoch 22/70\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.5221 - accuracy: 0.7117 - val_loss: 0.6015 - val_accuracy: 0.7200\n",
      "Epoch 23/70\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.5141 - accuracy: 0.7133 - val_loss: 0.6004 - val_accuracy: 0.7200\n",
      "Epoch 24/70\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.5062 - accuracy: 0.7133 - val_loss: 0.6000 - val_accuracy: 0.7200\n",
      "Epoch 25/70\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.4981 - accuracy: 0.7133 - val_loss: 0.5994 - val_accuracy: 0.7200\n",
      "Epoch 26/70\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.4900 - accuracy: 0.7150 - val_loss: 0.5990 - val_accuracy: 0.7200\n",
      "Epoch 27/70\n",
      "19/19 [==============================] - 0s 11ms/step - loss: 0.4818 - accuracy: 0.7167 - val_loss: 0.5988 - val_accuracy: 0.7200\n",
      "Epoch 28/70\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.4736 - accuracy: 0.7217 - val_loss: 0.5986 - val_accuracy: 0.7200\n",
      "Epoch 29/70\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.4652 - accuracy: 0.7250 - val_loss: 0.5988 - val_accuracy: 0.7200\n",
      "Epoch 30/70\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4567 - accuracy: 0.7317 - val_loss: 0.5991 - val_accuracy: 0.7200\n",
      "Epoch 31/70\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4481 - accuracy: 0.7400 - val_loss: 0.5994 - val_accuracy: 0.7200\n",
      "Epoch 32/70\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.4393 - accuracy: 0.7467 - val_loss: 0.5998 - val_accuracy: 0.7200\n",
      "Epoch 33/70\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.4306 - accuracy: 0.7633 - val_loss: 0.6002 - val_accuracy: 0.7200\n",
      "Epoch 34/70\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.4216 - accuracy: 0.7700 - val_loss: 0.6008 - val_accuracy: 0.7200\n",
      "Epoch 35/70\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.4126 - accuracy: 0.7883 - val_loss: 0.6015 - val_accuracy: 0.7200\n",
      "Epoch 36/70\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.4037 - accuracy: 0.8083 - val_loss: 0.6023 - val_accuracy: 0.7200\n",
      "Epoch 37/70\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.3947 - accuracy: 0.8200 - val_loss: 0.6032 - val_accuracy: 0.7200\n",
      "Epoch 38/70\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.3857 - accuracy: 0.8517 - val_loss: 0.6040 - val_accuracy: 0.7200\n",
      "Epoch 39/70\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.3767 - accuracy: 0.8633 - val_loss: 0.6049 - val_accuracy: 0.7200\n",
      "Epoch 40/70\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.3676 - accuracy: 0.8817 - val_loss: 0.6061 - val_accuracy: 0.7100\n",
      "Epoch 41/70\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.3586 - accuracy: 0.8933 - val_loss: 0.6071 - val_accuracy: 0.7100\n",
      "Epoch 42/70\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.3495 - accuracy: 0.8967 - val_loss: 0.6082 - val_accuracy: 0.7050\n",
      "Epoch 43/70\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.3405 - accuracy: 0.9050 - val_loss: 0.6095 - val_accuracy: 0.7050\n",
      "Epoch 44/70\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.3315 - accuracy: 0.9100 - val_loss: 0.6110 - val_accuracy: 0.7100\n",
      "Epoch 45/70\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.3225 - accuracy: 0.9233 - val_loss: 0.6123 - val_accuracy: 0.7100\n",
      "Epoch 46/70\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.3135 - accuracy: 0.9333 - val_loss: 0.6138 - val_accuracy: 0.7100\n",
      "Epoch 47/70\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.3047 - accuracy: 0.9383 - val_loss: 0.6153 - val_accuracy: 0.7100\n",
      "Epoch 48/70\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.2960 - accuracy: 0.9500 - val_loss: 0.6169 - val_accuracy: 0.7100\n",
      "Epoch 49/70\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.2873 - accuracy: 0.9500 - val_loss: 0.6187 - val_accuracy: 0.7100\n",
      "Epoch 50/70\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.2788 - accuracy: 0.9567 - val_loss: 0.6205 - val_accuracy: 0.7100\n",
      "Epoch 51/70\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.2703 - accuracy: 0.9600 - val_loss: 0.6223 - val_accuracy: 0.7050\n",
      "Epoch 52/70\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2621 - accuracy: 0.9650 - val_loss: 0.6244 - val_accuracy: 0.7000\n",
      "Epoch 53/70\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.2538 - accuracy: 0.9683 - val_loss: 0.6264 - val_accuracy: 0.7000\n",
      "Epoch 54/70\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.2458 - accuracy: 0.9683 - val_loss: 0.6285 - val_accuracy: 0.7000\n",
      "Epoch 55/70\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.2378 - accuracy: 0.9683 - val_loss: 0.6308 - val_accuracy: 0.7000\n",
      "Epoch 56/70\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.2302 - accuracy: 0.9733 - val_loss: 0.6330 - val_accuracy: 0.6950\n",
      "Epoch 57/70\n",
      "19/19 [==============================] - 0s 14ms/step - loss: 0.2225 - accuracy: 0.9733 - val_loss: 0.6353 - val_accuracy: 0.6900\n",
      "Epoch 58/70\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.2151 - accuracy: 0.9783 - val_loss: 0.6378 - val_accuracy: 0.6950\n",
      "Epoch 59/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 13ms/step - loss: 0.2077 - accuracy: 0.9800 - val_loss: 0.6405 - val_accuracy: 0.6900\n",
      "Epoch 60/70\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.2005 - accuracy: 0.9833 - val_loss: 0.6432 - val_accuracy: 0.6900\n",
      "Epoch 61/70\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.1936 - accuracy: 0.9833 - val_loss: 0.6460 - val_accuracy: 0.6900\n",
      "Epoch 62/70\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.1868 - accuracy: 0.9833 - val_loss: 0.6490 - val_accuracy: 0.6900\n",
      "Epoch 63/70\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.1803 - accuracy: 0.9833 - val_loss: 0.6518 - val_accuracy: 0.6850\n",
      "Epoch 64/70\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.1738 - accuracy: 0.9850 - val_loss: 0.6549 - val_accuracy: 0.6800\n",
      "Epoch 65/70\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.1675 - accuracy: 0.9850 - val_loss: 0.6580 - val_accuracy: 0.6800\n",
      "Epoch 66/70\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.1613 - accuracy: 0.9850 - val_loss: 0.6613 - val_accuracy: 0.6800\n",
      "Epoch 67/70\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.1554 - accuracy: 0.9850 - val_loss: 0.6645 - val_accuracy: 0.6800\n",
      "Epoch 68/70\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.1496 - accuracy: 0.9850 - val_loss: 0.6680 - val_accuracy: 0.6800\n",
      "Epoch 69/70\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.1439 - accuracy: 0.9850 - val_loss: 0.6713 - val_accuracy: 0.6800\n",
      "Epoch 70/70\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.1384 - accuracy: 0.9850 - val_loss: 0.6752 - val_accuracy: 0.6750\n"
     ]
    }
   ],
   "source": [
    "history=simple_nn.fit(train_tfidf2,np.array(train_qid),epochs=70,batch_size=32,\n",
    "                     validation_data=(devtest_tfidf2,np.array(devtest_qid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "x1= range(1,len(loss)+1)\n",
    "\n",
    "plt.plot(x1,loss,'bo',label=\"loss in Training\")\n",
    "plt.plot(x1,val_loss,'b1',label=\"Validation loss\")\n",
    "plt.title('Loss of Training and Validation')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xlabel(\"loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0NeK3gM0dC9"
   },
   "source": [
    "# Task 4 (5 marks): Recurrent NN\n",
    "\n",
    "Use the files `training.csv`, `dev_test.csv`, and `test.csv` provided by us.\n",
    "\n",
    "Implement a more complex neural network that is composed of the following layers:\n",
    "\n",
    "* An embedding layer that generates embedding vectors of the sentence text with 35 dimensions.\n",
    "* A LSTM layer. You need to determine the size of this LSTM layer, and the text length limit (if needed).\n",
    "* The final output layer with one cell for binary classification, as in task 3.\n",
    "\n",
    "Train the model with the training data, use the dev_test set to determine a good size of the LSTM layer and an appropriate length limit (if needed), and report the final results using the test set. Again, remember to use the test set only after you have determined the optimal parameters of the LSTM layer.\n",
    "\n",
    "Based on your experiments, comment on whether this system is better than the systems developed in the previous tasks.\n",
    "\n",
    "The breakdown of marks is as follows:\n",
    "\n",
    "* **1 mark** if the NN model has the correct layers, the correct activation functions, and the correct loss function.\n",
    "* **1 mark** if the code passes the sentence text to the model correctly. The documentation needs to explain what decisions had to be made to process long sentences. In particular, did you need to truncate the input text, and how did you determine the length limit?\n",
    "* **1 mark** if the code returns the IDs of the *n* sentences that have the highest prediction score in the given question.\n",
    "* **1 mark** if the notebook reports the F1 scores of the test sets and comments on the results.\n",
    "* **1 mark** for good coding and documentation in this task. In particular, the code and results must include evidence that shows your choice of best size of the LSTM layer. The explanations must be clear and concise. To make this task less time-consuming, use $n=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8RRCWeQTrPl"
   },
   "outputs": [],
   "source": [
    "# Write your code and answers here. Feel free to add more code and markdown cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppkBsuB_0dC9"
   },
   "source": [
    "# Submission of results\n",
    "\n",
    "Your submission should consist of this Jupyter notebook with all your code and explanations inserted into the notebook as text cells. **The notebook should contain the output of the runs. All code should run. Code with syntax errors or code without output will not be assessed.**\n",
    "\n",
    "**Do not submit multiple files. If you feel you need to submit multiple files, please contact Diego.Molla-Aliod@mq.edu.au first.**\n",
    "\n",
    "Examine the text cells of this notebook so that you can have an idea of how to format text for good visual impact. You can also read this useful [guide to the MarkDown notation](https://daringfireball.net/projects/markdown/syntax),  which explains the format of the text cells.\n",
    "\n",
    "Each task specifies a number of marks. The final mark of the assignment is the sum of all the marks of each individual task.\n",
    "\n",
    "By submitting this assignment you are acknowledging that this is your own work. Any submissions that break the code of academic honesty will be penalised as per [the academic integrity policy](https://policies.mq.edu.au/document/view.php?id=3).\n",
    "\n",
    "Late submissions **will not be accepted** without an approved [Special Consideration](http://from.mq.edu.au/MT0X0E0FUrrU200rm0JB0U0) request.  Assessments submitted after the due date will receive a mark of **zero**."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "A2_solution.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a7b63e7410c98f344f02082f10d790581d1dba1eeb1c8fe30f342f6109f0429e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
